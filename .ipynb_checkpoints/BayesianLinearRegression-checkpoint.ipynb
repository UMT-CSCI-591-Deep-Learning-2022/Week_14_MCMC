{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_IiiF75017j"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgJekUTeVNI-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTYuV6v91IE5"
   },
   "source": [
    "This specific notebook focuses on using MCMC methods to determine the coefficients of a polynomial.  In other words, today we seek to perform [Bayesian linear regression](https://en.wikipedia.org/wiki/Bayesian_linear_regression) using two different MCMC algorithms discussed in class, Metropolis-Hastings (MH), and Hybrid Monte-Carlo (HMC).  The former is considered the standard for MCMC algorithms and is often the basis upon which other MCMC algorithms have been developed.  Specifically here, HMC is relevant due to its utilization of gradients, which feature prominently in Neural Networks.  As such, HMC is particuluarly suited to adoption in machine learning problems.\n",
    "\n",
    "## Data Generation\n",
    "\n",
    "The function below generates points from a polynomial of a specified order.  The coefficients of this polynomial are chosen randomly between -10 and 10, and will be the features in the MCMC models experimented with.  The function returns both the point values, for plotting and training within the later MCMC models, and the generated coefficients for comparison to the results of the MCMC models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c2bQFd_u1Cr6"
   },
   "outputs": [],
   "source": [
    "#generate data from a polynomial with random gaussian noise\n",
    "def curve_gen(x,order,sigma_obs):\n",
    "  coeffs = []\n",
    "  #generate curve\n",
    "  return_val = 0\n",
    "  for exp in range(order+1):\n",
    "    coeff = np.random.rand()*2 - 1\n",
    "    coeffs.append(coeff) #store for checking results against later\n",
    "    return_val += (coeff*(x**exp))\n",
    "\n",
    "  #add randomness\n",
    "  for i in range(len(return_val)):\n",
    "    return_val[i] += np.random.normal(scale=sigma_obs)\n",
    "  return return_val,coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AmpvVkhq2ndg"
   },
   "source": [
    "Below the order of the polynomial and number of points plotted can be chosen.  After this the function $\\tt{curve\\_gen}$ defined above is called and the data for later experimentation generated.  The coefficients are saved in the variable $\\tt{act\\_coeffs}$ for later comparison.  Note too that a seed is given to numpy to enable reproducability.  This line may be removed if a different result is desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "kZXIFmcnTdsm",
    "outputId": "e8b9f2a6-0b4e-46d6-c846-10ccef6146e6"
   },
   "outputs": [],
   "source": [
    "#for reproducability\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# The order of the polynomial\n",
    "order = 1\n",
    "\n",
    "# The number of data points to simulate\n",
    "num_pts = 10\n",
    "\n",
    "# The x locations\n",
    "x = np.linspace(-1,1,num_pts)\n",
    "\n",
    "# The observation noise\n",
    "sigma_obs = 0.1\n",
    "\n",
    "y,act_coeffs = curve_gen(x,order,sigma_obs)\n",
    "\n",
    "print(act_coeffs)\n",
    "\n",
    "plt.plot(x,y,'.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BNtdur6w34v_"
   },
   "source": [
    "## Bayesian Functions\n",
    "Here functions are defined for the calculation of the posterior distribution.  In particular we need both the prior and the likelihood for the calculation of the posterior.  According to Bayes' theorem, knowing these two terms along with the evidence (i.e. probability of the specific data being chosen) is all that is needed to calculate the posterior.  Bayes' theorem according to the variables defined in this document is presented below.\n",
    "\n",
    "$$p(\\beta|X,Y) = \\cfrac{p(Y|X,ùú∑)p(ùú∑)}{p(Y|X)}$$\n",
    "\n",
    "**Question: Why is there no equation for the evidence below?  Why is it not needed with MCMC methods?**\n",
    "\n",
    "With linear regression (not only Bayesian linear regression), it's often useful to calculate a mathematical object known as the design matrix.\n",
    "\n",
    "A general polynomial curve of order $i$ can be constructed as below, where $\\beta_i$ is the coefficient on $x^i$.  Note that $\\beta_0$ is the y-intercept.\n",
    "\n",
    "$$y = \\sum_{i=0}^i\\beta_ix^i$$\n",
    "\n",
    "Rather than using a sum, this equality can be equivalently written using vector multiplication.\n",
    "\n",
    "$$y = ùùì^Tùú∑$$\n",
    "\n",
    "$$ùùì = \\begin{pmatrix}\n",
    "x^0 \\\\\n",
    "x^1 \\\\\n",
    "\\vdots\\\\\n",
    "x^i\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "x^1 \\\\\n",
    "\\vdots\\\\\n",
    "x^i\n",
    "\\end{pmatrix}, \n",
    "ùú∑ = \\begin{pmatrix}\n",
    "\\beta_0 \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots\\\\\n",
    "\\beta_i\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "In the case of multiple datapoints the vector $ùùì$ becomes the design matrix $ùú±$.\n",
    "\n",
    "$$\n",
    "ùú± = \n",
    "\\begin{pmatrix}\n",
    "1 & x_0^1 & . . . & x_0^i\\\\\n",
    "1 & x_1^1 & . . . & x_1^i\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_i^1 & . . . & x_i^i\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Therefore, $Y = ùú±ùú∑$ in the case of multiple data points.  It is left to the student to write the function $\\tt{design\\_mat}$, which is intended to return a design matrix.  As well, the return function for the likelihood is left empty.  This needs to return the likelihood of $Y$ given $ùú∑$ and $X$.  As a hint, it's useful to consult both the other functions compelted below as well as other online sources.  [This](https://alpopkes.com/posts/machine_learning/bayesian_linear_regression/) page, [this](https://letianzj.github.io/mcmc-linear-regression.html) page, and [this](https://letianzj.github.io/bayesian-linear-regression.html) page are recommended.  (Note: the design matrix is sometimes referred to as a Vandermond matrix; check out the numpy function vander).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_mat(x, order):\n",
    "  #TODO: Write the function which will return the design matrix.\n",
    "  return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to specify our models of the likelihood and prior distribution.  For the likelihood, you should use \n",
    "$$\n",
    "P(Y|X,\\beta) = \\prod_{i=1}^n \\frac{1}{\\sigma \\sqrt{2\\pi}} \\exp{-\\frac{1}{2} \\left( \\frac{Y_i - X_i \\beta}{\\sigma} \\right)^2},\n",
    "$$\n",
    "which has the logarithm\n",
    "$$\n",
    "\\log P(Y|X,\\beta) = -\\frac{n}{2}(\\ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (Y_i - X_i \\beta)^2\n",
    "$$\n",
    "Note that this is just a simple sum of squares (with some extra constants).\n",
    "\n",
    "For the prior, you should also assume that the parameters $\\beta$ are, a priori, normally distributed with zero mean and given prior standard deviation $\\sigma^2_{prior}$ \n",
    "$$\n",
    "\\log P(\\beta_i) = -\\frac{1}{2}(\\ln(2\\pi\\sigma^2_{prior,i}) - \\frac{1}{2\\sigma^2_{prior,i}} \\beta_i^2 .\n",
    "$$\n",
    "You might be able to convince yourself that this looks rather like L2 regularization.\n",
    "\n",
    "Next, implement functions that accept $X$, $Y$, and $\\beta$ and evaluate the log-likelihood, log-prior, and log-posterior (up to a constant of proportionality).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7lmBWphwg8mA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def log_likelihood(beta,x,y,sigma_obs):\n",
    "  return\n",
    "\n",
    "def log_prior(beta,sigma_prior):\n",
    "  return \n",
    "\n",
    "def log_posterior(beta,x,y,sigma_obs,sigma_prior):\n",
    "  log_like = log_likelihood(beta,x,y,sigma_obs)\n",
    "  log_pr = log_prior(beta,sigma_prior)\n",
    "  return log_like + log_pr\n",
    "\n",
    "\n",
    "## Strongly suggest doing some unit testing here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABZheP01D0fQ"
   },
   "source": [
    "## The Algorithms!\n",
    "\n",
    "With the Bayesian definitions above it's now time to define our algorithms.  The task today is to implement the Metropolis-Hastings algoirthm.  As a bonus you can try to implement Hybrid Monte-Carlo.  Very little code is provided for you.  While I was implementing these, the best source was the pseudo-code from the paper, followed closely by the links provided above.  However, if you get stuck feel free to ask one of us in class or take a look at the key provided!\n",
    "\n",
    "**Question: Refer to Figure 5 in the paper, specifically the 5th line with the minimum function.  Is it necessary to have the proposal distribution q when calculating the acceptance rate?  If so why, if not why not?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu_FV-krg8uu"
   },
   "outputs": [],
   "source": [
    "\n",
    "#TODO: Implement Metropolis-Hastings algorithm\n",
    "def mh(order,steps=10000,stepsize=0.1):\n",
    "  betas = [np.zeros(order+1)] #initial guess\n",
    "  for i in range(steps):\n",
    "    if (i+1)%(steps/100) == 0:print(\"Step \",i+1,\"/\",steps,sep='') #print steps to check progress\n",
    "\n",
    "    #get a random uniform sample, what should the bounds be?\n",
    "    \n",
    "    #there should be a new beta proposed in here\n",
    "    \n",
    "    #what should the acceptance look like?\n",
    "    \n",
    "    #do we need to look at the acceptance of all features individually or together?\n",
    "  return betas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vvAnZe6HLi8"
   },
   "source": [
    "Below run your implemented Metropolis-Hastings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vdj2w6yoFguC",
    "outputId": "3663ff98-a86f-4b12-f5c3-bc230ce3a4db"
   },
   "outputs": [],
   "source": [
    "betas = mh(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0ALjYm0FYUR"
   },
   "source": [
    "Plot the results of your Metropolis-Hastings implementation.\n",
    "\n",
    "Below is a histogram of your MCMC results along with a red line indicating the true coefficient value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 294
    },
    "id": "fJXf-HToKgGR",
    "outputId": "38a4d830-5a98-4cc8-ccc9-2c4ddf477988"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, order+1)\n",
    "fig.suptitle('Learned Distributions')\n",
    "for i in range(len(axes)):\n",
    "  axes[i].hist(np.array(betas)[:,i],density=True,range=(-1,1),bins=20)\n",
    "  axes[i].axvline(x=act_coeffs[i],color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9eoaibmLv9Z"
   },
   "source": [
    "Below a function $\\tt{coeff\\_plot}$ is provided which takes in a series of coefficients and returns y-values for plotting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LPKlDPxd9T8P"
   },
   "outputs": [],
   "source": [
    "def coeff_plot(coeffs,x):\n",
    "  #generate curve\n",
    "  return_val = 0\n",
    "  for exp in range(len(coeffs)):\n",
    "    return_val += (coeffs[exp]*(x**exp))\n",
    "  return return_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1-2beY_O_sT"
   },
   "source": [
    "First, **plot the random walk of MH algorithm throughout the feature space.** (i.e. plot the entire sequence of $\\beta$ values as a scatter plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "NIViqwJ7PcsX",
    "outputId": "02dfe111-53a3-4ba9-b874-7be7f9393346"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBX601wXO3Rg"
   },
   "source": [
    "Next, **plot the posterior predictive distribution, i.e. make a plot of x versus y, overlaid with the predicted y for each sample of $\\beta$ drawn via MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "NU14HFkV9Jfy",
    "outputId": "45e43e6b-ab3f-4133-acdb-640b924f15e9"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'.')\n",
    "\n",
    "plt.plot(x,coeff_plot(act_coeffs,x),label='Actual')\n",
    "\n",
    "# for beta in samples, plot the regression line\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Hybrid MC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LwI6_zehSYCM"
   },
   "source": [
    "Before implementing HMC you will need to define the gradient of the posterior.  This is done for you.  However, the below function relies heavily on the gradient of the likelihood which is left mostly undefined.\n",
    "\n",
    "Hint: What is the derivative of a normal distribution?  How do you account for the multiple features?  Do we already have easy access to such an object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adr0_KA6U8ug"
   },
   "outputs": [],
   "source": [
    "def likelihood_grad(beta,x,y):\n",
    "  #order is one less than the length of beta\n",
    "\n",
    "  #what is the derivative of a normal distribution?\n",
    "    \n",
    "  #how are we going to account for gradient with respect to multiple features?\n",
    "\n",
    "  return None\n",
    "\n",
    "def posterior_grad(beta,x,y):\n",
    "  like = likelihood_grad(beta,x,y)\n",
    "  pr = prior(beta)\n",
    "  return np.sum(like,axis=1)+pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o6w5beMC9Jqm"
   },
   "outputs": [],
   "source": [
    "#TODO: Implement Hybrid Monte-Carlo algorithm\n",
    "def hmc(order, steps=10000,L=10,rho=0.1):\n",
    "  betas = [np.zeros(order+1)]#initial guess\n",
    "  for i in range(steps):\n",
    "    if (i+1)%(steps/100) == 0:print(\"Step \",i+1,\"/\",steps,sep='')#print steps to check progress\n",
    "    #now you need to sample from both a uniform and normal distribution\n",
    "    \n",
    "    #do you need a proposal distribution?  how are we going to get a proposed beta?\n",
    "    \n",
    "    #does anything special need to be done with rho?\n",
    "    \n",
    "    #how has the acceptance changed?\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIo_qPwwTD-W"
   },
   "source": [
    "Below run your implemented Hybrid Monte-Carlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFRnA7h0H_dt",
    "outputId": "beca37c4-8601-4416-bed1-892c3a170914"
   },
   "outputs": [],
   "source": [
    "betas = hmc(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6T5DygcETWZn"
   },
   "source": [
    "Plot the random walk of HMC throughout the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRpRKM30PnYj"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(betas)\n",
    "g = sb.PairGrid(df)\n",
    "g.map_offdiag(plt.scatter,s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oLjIMmM0TY63"
   },
   "source": [
    "Compare the results of HMC to the actual coefficients used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Ip7_rJ3RPnb9",
    "outputId": "b733769d-46c7-4a79-ea36-a01cc12daf9d"
   },
   "outputs": [],
   "source": [
    "plt.plot(x,y,'.')\n",
    "plt.plot(x,coeff_plot(gen_coeffs(betas),x),label='MCMC')\n",
    "plt.plot(x,coeff_plot(act_coeffs,x),label='Actual')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianLinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
